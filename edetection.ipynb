{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRrFJ691qlObaVY3rBIvuv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fyousseff/youss/blob/master/edetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPtQzslfLM8W"
      },
      "source": [
        "pip list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7HlSlxeLQrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae05e43-4f23-4f10-bf00-74f2c736188d"
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 62663, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 62663 (delta 18), reused 53 (delta 14), pack-reused 62604\u001b[K\n",
            "Receiving objects: 100% (62663/62663), 574.53 MiB | 27.82 MiB/s, done.\n",
            "Resolving deltas: 100% (43653/43653), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyPKc6vbFv6E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZdHvZBrMXMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49bf090f-5629-4fb0-87c5-6d73e7c6db69"
      },
      "source": [
        "cd /content/models/research"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxIJIoGYNQow"
      },
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHPxbE_uNRC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "310ec0f1-c061-4335-ad38-59819fcceb31"
      },
      "source": [
        "!git clone https://github.com/cocodataset/cocoapi.git\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cocoapi'...\n",
            "remote: Enumerating objects: 975, done.\u001b[K\n",
            "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975\u001b[K\n",
            "Receiving objects: 100% (975/975), 11.72 MiB | 21.51 MiB/s, done.\n",
            "Resolving deltas: 100% (576/576), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGIdbGZ1NSft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ece2f7b-37e4-4275-ad16-ade10f08c678"
      },
      "source": [
        "cd cocoapi/PythonAPI"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/cocoapi/PythonAPI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZVhYVXKNUUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28daf9f0-0c41-4688-c005-c19978831ef0"
      },
      "source": [
        "!make"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python setup.py build_ext --inplace\n",
            "running build_ext\n",
            "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
            "/usr/local/lib/python3.7/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/models/research/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "building 'pycocotools._mask' extension\n",
            "creating build\n",
            "creating build/common\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c ../common/maskApi.c -o build/temp.linux-x86_64-3.7/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
            "       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
            "                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
            "       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
            "                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
            "   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
            "   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
            "                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
            "     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
            "                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToBbox\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:141:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kxp\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "       if(j%2==0) xp=x; else if\u001b[01;35m\u001b[K(\u001b[m\u001b[Kxp<x) { ys=0; ye=h-1; }\n",
            "                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/../common/maskApi.o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -o build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so -> pycocotools\n",
            "rm -rf build\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbMRVcY7NV_g"
      },
      "source": [
        "cp -r pycocotools /content/models/research"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzPFTvflNatO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac52cfe8-d6e8-4a3c-b723-7b115b1833fe"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIsePVSmNeQR"
      },
      "source": [
        "cp object_detection/packages/tf2/setup.py ."
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFD3EiyJNkIj"
      },
      "source": [
        "!python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgeq3qvXNmy0"
      },
      "source": [
        "!python object_detection/builders/model_builder_tf2_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9xnbWI8UAdy"
      },
      "source": [
        "create files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TN7yJbRHXjW"
      },
      "source": [
        "cd /content/\n",
        "import os\n",
        "lst = ['pre-trained-models','models','images/train','images/test','exported-models','annotations']\n",
        "for f in lst:\n",
        "  os.makedirs(os.path.join('workspace/training_demo', f))\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhO7_RukT_1H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lidb7zbmOGBK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36a6940-13c5-40db-a3f4-ee611772694a"
      },
      "source": [
        "cd /content/workspace/training_demo/pre-trained-models\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "! tar -xvf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-15 12:06:03--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 64.233.191.128, 2607:f8b0:4001:c0c::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|64.233.191.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 244817203 (233M) [application/x-tar]\n",
            "Saving to: ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz’\n",
            "\n",
            "ssd_resnet50_v1_fpn 100%[===================>] 233.48M   211MB/s    in 1.1s    \n",
            "\n",
            "2021-09-15 12:06:04 (211 MB/s) - ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz’ saved [244817203/244817203]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYZkPMGNJtx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd0b357-15c9-45ff-aad7-83f3c71fc633"
      },
      "source": [
        "cd /content/workspace/training_demo"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created the TFRecord file: /content/workspace/training_demo/annotations/train.record\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaATYcOxUnc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ffa87a4-05c2-427e-c672-328172c239d9"
      },
      "source": [
        "!python generate_tfrecord.py -x /content/workspace/training_demo/images/train -l /content/workspace/training_demo/annotations/label_map.pbtxt -o /content/workspace/training_demo/annotations/train.record\n",
        "!python generate_tfrecord.py -x /content/workspace/training_demo/images/test -l /content/workspace/training_demo/annotations/label_map.pbtxt -o /content/workspace/training_demo/annotations/test.record"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created the TFRecord file: /content/workspace/training_demo/annotations/test.record\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3J9tFOeVoB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883a1d3b-6480-4820-a25b-3513788fff1d"
      },
      "source": [
        "!python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-09-15 12:10:04.099669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 12:10:04.114758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 12:10:04.115642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 12:10:04.117252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 12:10:04.118108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 12:10:04.119032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 12:10:04.595731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 12:10:04.596608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 12:10:04.597406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 12:10:04.598135: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-09-15 12:10:04.598200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10819 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I0915 12:10:04.603515 139846094190464 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: None\n",
            "I0915 12:10:04.608289 139846094190464 config_util.py:552] Maybe overwriting train_steps: None\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0915 12:10:04.608478 139846094190464 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:558: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W0915 12:10:04.736702 139846094190464 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:558: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['annotations/train.record']\n",
            "I0915 12:10:04.749297 139846094190464 dataset_builder.py:163] Reading unweighted datasets: ['annotations/train.record']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['annotations/train.record']\n",
            "I0915 12:10:04.749543 139846094190464 dataset_builder.py:80] Reading record datasets for input file: ['annotations/train.record']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I0915 12:10:04.749687 139846094190464 dataset_builder.py:81] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0915 12:10:04.749825 139846094190464 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
            "W0915 12:10:04.766760 139846094190464 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0915 12:10:04.795182 139846094190464 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0915 12:10:13.127855 139846094190464 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0915 12:10:16.991189 139846094190464 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0915 12:10:19.052163 139846094190464 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "2021-09-15 12:10:21.768570: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:401: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
            "2021-09-15 12:10:50.053751: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8004\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0915 12:11:19.036644 139846094190464 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0915 12:11:19.038058 139846094190464 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0915 12:11:19.041170 139846094190464 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0915 12:11:19.042332 139846094190464 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0915 12:11:19.045394 139846094190464 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0915 12:11:19.046513 139846094190464 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0915 12:11:19.049963 139846094190464 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0915 12:11:19.051153 139846094190464 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0915 12:11:19.053488 139846094190464 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0915 12:11:19.054660 139846094190464 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:617: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "W0915 12:11:20.291185 139842837956352 deprecation.py:548] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:617: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "2021-09-15 12:12:22.310194: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "2021-09-15 12:12:22.443851: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.14GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "INFO:tensorflow:Step 100 per-step time 3.540s\n",
            "I0915 12:17:13.795105 139846094190464 model_lib_v2.py:700] Step 100 per-step time 3.540s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.06503499,\n",
            " 'Loss/localization_loss': 0.022601806,\n",
            " 'Loss/regularization_loss': 0.2621344,\n",
            " 'Loss/total_loss': 0.3497712,\n",
            " 'learning_rate': 0.014666351}\n",
            "I0915 12:17:13.803400 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.06503499,\n",
            " 'Loss/localization_loss': 0.022601806,\n",
            " 'Loss/regularization_loss': 0.2621344,\n",
            " 'Loss/total_loss': 0.3497712,\n",
            " 'learning_rate': 0.014666351}\n",
            "INFO:tensorflow:Step 200 per-step time 2.895s\n",
            "I0915 12:22:03.169630 139846094190464 model_lib_v2.py:700] Step 200 per-step time 2.895s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.061925698,\n",
            " 'Loss/localization_loss': 0.01511756,\n",
            " 'Loss/regularization_loss': 0.2593067,\n",
            " 'Loss/total_loss': 0.33634996,\n",
            " 'learning_rate': 0.0159997}\n",
            "I0915 12:22:03.170070 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.061925698,\n",
            " 'Loss/localization_loss': 0.01511756,\n",
            " 'Loss/regularization_loss': 0.2593067,\n",
            " 'Loss/total_loss': 0.33634996,\n",
            " 'learning_rate': 0.0159997}\n",
            "INFO:tensorflow:Step 300 per-step time 2.891s\n",
            "I0915 12:26:52.226752 139846094190464 model_lib_v2.py:700] Step 300 per-step time 2.891s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.044905692,\n",
            " 'Loss/localization_loss': 0.005957289,\n",
            " 'Loss/regularization_loss': 0.25624552,\n",
            " 'Loss/total_loss': 0.3071085,\n",
            " 'learning_rate': 0.01733305}\n",
            "I0915 12:26:52.227169 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.044905692,\n",
            " 'Loss/localization_loss': 0.005957289,\n",
            " 'Loss/regularization_loss': 0.25624552,\n",
            " 'Loss/total_loss': 0.3071085,\n",
            " 'learning_rate': 0.01733305}\n",
            "INFO:tensorflow:Step 400 per-step time 2.892s\n",
            "I0915 12:31:41.457141 139846094190464 model_lib_v2.py:700] Step 400 per-step time 2.892s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.02808799,\n",
            " 'Loss/localization_loss': 0.011916543,\n",
            " 'Loss/regularization_loss': 0.252933,\n",
            " 'Loss/total_loss': 0.29293755,\n",
            " 'learning_rate': 0.0186664}\n",
            "I0915 12:31:41.457588 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.02808799,\n",
            " 'Loss/localization_loss': 0.011916543,\n",
            " 'Loss/regularization_loss': 0.252933,\n",
            " 'Loss/total_loss': 0.29293755,\n",
            " 'learning_rate': 0.0186664}\n",
            "INFO:tensorflow:Step 500 per-step time 2.896s\n",
            "I0915 12:36:31.043992 139846094190464 model_lib_v2.py:700] Step 500 per-step time 2.896s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.042549834,\n",
            " 'Loss/localization_loss': 0.0067011607,\n",
            " 'Loss/regularization_loss': 0.24939159,\n",
            " 'Loss/total_loss': 0.29864258,\n",
            " 'learning_rate': 0.01999975}\n",
            "I0915 12:36:31.044382 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.042549834,\n",
            " 'Loss/localization_loss': 0.0067011607,\n",
            " 'Loss/regularization_loss': 0.24939159,\n",
            " 'Loss/total_loss': 0.29864258,\n",
            " 'learning_rate': 0.01999975}\n",
            "INFO:tensorflow:Step 600 per-step time 2.894s\n",
            "I0915 12:41:20.452540 139846094190464 model_lib_v2.py:700] Step 600 per-step time 2.894s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.065513305,\n",
            " 'Loss/localization_loss': 0.022505797,\n",
            " 'Loss/regularization_loss': 0.24574453,\n",
            " 'Loss/total_loss': 0.33376363,\n",
            " 'learning_rate': 0.0213331}\n",
            "I0915 12:41:20.452964 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.065513305,\n",
            " 'Loss/localization_loss': 0.022505797,\n",
            " 'Loss/regularization_loss': 0.24574453,\n",
            " 'Loss/total_loss': 0.33376363,\n",
            " 'learning_rate': 0.0213331}\n",
            "INFO:tensorflow:Step 700 per-step time 2.896s\n",
            "I0915 12:46:10.039044 139846094190464 model_lib_v2.py:700] Step 700 per-step time 2.896s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.020932231,\n",
            " 'Loss/localization_loss': 0.0045160325,\n",
            " 'Loss/regularization_loss': 0.24191992,\n",
            " 'Loss/total_loss': 0.2673682,\n",
            " 'learning_rate': 0.02266645}\n",
            "I0915 12:46:10.039422 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.020932231,\n",
            " 'Loss/localization_loss': 0.0045160325,\n",
            " 'Loss/regularization_loss': 0.24191992,\n",
            " 'Loss/total_loss': 0.2673682,\n",
            " 'learning_rate': 0.02266645}\n",
            "INFO:tensorflow:Step 800 per-step time 2.890s\n",
            "I0915 12:50:59.042606 139846094190464 model_lib_v2.py:700] Step 800 per-step time 2.890s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.027210373,\n",
            " 'Loss/localization_loss': 0.0028210871,\n",
            " 'Loss/regularization_loss': 0.23780221,\n",
            " 'Loss/total_loss': 0.26783368,\n",
            " 'learning_rate': 0.023999799}\n",
            "I0915 12:50:59.043028 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.027210373,\n",
            " 'Loss/localization_loss': 0.0028210871,\n",
            " 'Loss/regularization_loss': 0.23780221,\n",
            " 'Loss/total_loss': 0.26783368,\n",
            " 'learning_rate': 0.023999799}\n",
            "INFO:tensorflow:Step 900 per-step time 2.894s\n",
            "I0915 12:55:48.420639 139846094190464 model_lib_v2.py:700] Step 900 per-step time 2.894s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.036068067,\n",
            " 'Loss/localization_loss': 0.0066408725,\n",
            " 'Loss/regularization_loss': 0.23357041,\n",
            " 'Loss/total_loss': 0.27627936,\n",
            " 'learning_rate': 0.025333151}\n",
            "I0915 12:55:48.421023 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.036068067,\n",
            " 'Loss/localization_loss': 0.0066408725,\n",
            " 'Loss/regularization_loss': 0.23357041,\n",
            " 'Loss/total_loss': 0.27627936,\n",
            " 'learning_rate': 0.025333151}\n",
            "INFO:tensorflow:Step 1000 per-step time 2.893s\n",
            "I0915 13:00:37.757944 139846094190464 model_lib_v2.py:700] Step 1000 per-step time 2.893s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.04390679,\n",
            " 'Loss/localization_loss': 0.006297486,\n",
            " 'Loss/regularization_loss': 0.22919385,\n",
            " 'Loss/total_loss': 0.27939814,\n",
            " 'learning_rate': 0.0266665}\n",
            "I0915 13:00:37.758327 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.04390679,\n",
            " 'Loss/localization_loss': 0.006297486,\n",
            " 'Loss/regularization_loss': 0.22919385,\n",
            " 'Loss/total_loss': 0.27939814,\n",
            " 'learning_rate': 0.0266665}\n",
            "INFO:tensorflow:Step 1100 per-step time 2.914s\n",
            "I0915 13:05:29.181054 139846094190464 model_lib_v2.py:700] Step 1100 per-step time 2.914s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.06528924,\n",
            " 'Loss/localization_loss': 0.009106603,\n",
            " 'Loss/regularization_loss': 0.22470635,\n",
            " 'Loss/total_loss': 0.2991022,\n",
            " 'learning_rate': 0.02799985}\n",
            "I0915 13:05:29.181502 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.06528924,\n",
            " 'Loss/localization_loss': 0.009106603,\n",
            " 'Loss/regularization_loss': 0.22470635,\n",
            " 'Loss/total_loss': 0.2991022,\n",
            " 'learning_rate': 0.02799985}\n",
            "INFO:tensorflow:Step 1200 per-step time 2.892s\n",
            "I0915 13:10:18.420884 139846094190464 model_lib_v2.py:700] Step 1200 per-step time 2.892s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.013964384,\n",
            " 'Loss/localization_loss': 0.0025962966,\n",
            " 'Loss/regularization_loss': 0.22006777,\n",
            " 'Loss/total_loss': 0.23662846,\n",
            " 'learning_rate': 0.0293332}\n",
            "I0915 13:10:18.421279 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.013964384,\n",
            " 'Loss/localization_loss': 0.0025962966,\n",
            " 'Loss/regularization_loss': 0.22006777,\n",
            " 'Loss/total_loss': 0.23662846,\n",
            " 'learning_rate': 0.0293332}\n",
            "INFO:tensorflow:Step 1300 per-step time 2.895s\n",
            "I0915 13:15:07.924164 139846094190464 model_lib_v2.py:700] Step 1300 per-step time 2.895s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.027775813,\n",
            " 'Loss/localization_loss': 0.007584934,\n",
            " 'Loss/regularization_loss': 0.2155104,\n",
            " 'Loss/total_loss': 0.25087115,\n",
            " 'learning_rate': 0.03066655}\n",
            "I0915 13:15:07.924543 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.027775813,\n",
            " 'Loss/localization_loss': 0.007584934,\n",
            " 'Loss/regularization_loss': 0.2155104,\n",
            " 'Loss/total_loss': 0.25087115,\n",
            " 'learning_rate': 0.03066655}\n",
            "INFO:tensorflow:Step 1400 per-step time 2.892s\n",
            "I0915 13:19:57.133861 139846094190464 model_lib_v2.py:700] Step 1400 per-step time 2.892s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.024062172,\n",
            " 'Loss/localization_loss': 0.0036315103,\n",
            " 'Loss/regularization_loss': 0.21068802,\n",
            " 'Loss/total_loss': 0.23838171,\n",
            " 'learning_rate': 0.0319999}\n",
            "I0915 13:19:57.134263 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.024062172,\n",
            " 'Loss/localization_loss': 0.0036315103,\n",
            " 'Loss/regularization_loss': 0.21068802,\n",
            " 'Loss/total_loss': 0.23838171,\n",
            " 'learning_rate': 0.0319999}\n",
            "INFO:tensorflow:Step 1500 per-step time 2.896s\n",
            "I0915 13:24:46.700415 139846094190464 model_lib_v2.py:700] Step 1500 per-step time 2.896s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.029899297,\n",
            " 'Loss/localization_loss': 0.0035543067,\n",
            " 'Loss/regularization_loss': 0.20581649,\n",
            " 'Loss/total_loss': 0.23927009,\n",
            " 'learning_rate': 0.03333325}\n",
            "I0915 13:24:46.700775 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.029899297,\n",
            " 'Loss/localization_loss': 0.0035543067,\n",
            " 'Loss/regularization_loss': 0.20581649,\n",
            " 'Loss/total_loss': 0.23927009,\n",
            " 'learning_rate': 0.03333325}\n",
            "INFO:tensorflow:Step 1600 per-step time 2.892s\n",
            "I0915 13:29:35.865217 139846094190464 model_lib_v2.py:700] Step 1600 per-step time 2.892s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.020258503,\n",
            " 'Loss/localization_loss': 0.0018305298,\n",
            " 'Loss/regularization_loss': 0.20076169,\n",
            " 'Loss/total_loss': 0.22285073,\n",
            " 'learning_rate': 0.034666598}\n",
            "I0915 13:29:35.865679 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.020258503,\n",
            " 'Loss/localization_loss': 0.0018305298,\n",
            " 'Loss/regularization_loss': 0.20076169,\n",
            " 'Loss/total_loss': 0.22285073,\n",
            " 'learning_rate': 0.034666598}\n",
            "INFO:tensorflow:Step 1700 per-step time 2.896s\n",
            "I0915 13:34:25.452634 139846094190464 model_lib_v2.py:700] Step 1700 per-step time 2.896s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.05178829,\n",
            " 'Loss/localization_loss': 0.0038394874,\n",
            " 'Loss/regularization_loss': 0.19568895,\n",
            " 'Loss/total_loss': 0.25131673,\n",
            " 'learning_rate': 0.03599995}\n",
            "I0915 13:34:25.453040 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.05178829,\n",
            " 'Loss/localization_loss': 0.0038394874,\n",
            " 'Loss/regularization_loss': 0.19568895,\n",
            " 'Loss/total_loss': 0.25131673,\n",
            " 'learning_rate': 0.03599995}\n",
            "INFO:tensorflow:Step 1800 per-step time 2.895s\n",
            "I0915 13:39:14.933982 139846094190464 model_lib_v2.py:700] Step 1800 per-step time 2.895s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.021286333,\n",
            " 'Loss/localization_loss': 0.0036564698,\n",
            " 'Loss/regularization_loss': 0.19055095,\n",
            " 'Loss/total_loss': 0.21549377,\n",
            " 'learning_rate': 0.037333302}\n",
            "I0915 13:39:14.934533 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.021286333,\n",
            " 'Loss/localization_loss': 0.0036564698,\n",
            " 'Loss/regularization_loss': 0.19055095,\n",
            " 'Loss/total_loss': 0.21549377,\n",
            " 'learning_rate': 0.037333302}\n",
            "INFO:tensorflow:Step 1900 per-step time 2.895s\n",
            "I0915 13:44:04.380184 139846094190464 model_lib_v2.py:700] Step 1900 per-step time 2.895s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.029463105,\n",
            " 'Loss/localization_loss': 0.0025847522,\n",
            " 'Loss/regularization_loss': 0.18553036,\n",
            " 'Loss/total_loss': 0.21757823,\n",
            " 'learning_rate': 0.03866665}\n",
            "I0915 13:44:04.380556 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.029463105,\n",
            " 'Loss/localization_loss': 0.0025847522,\n",
            " 'Loss/regularization_loss': 0.18553036,\n",
            " 'Loss/total_loss': 0.21757823,\n",
            " 'learning_rate': 0.03866665}\n",
            "INFO:tensorflow:Step 2000 per-step time 2.892s\n",
            "I0915 13:48:53.536686 139846094190464 model_lib_v2.py:700] Step 2000 per-step time 2.892s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.01927965,\n",
            " 'Loss/localization_loss': 0.0028449497,\n",
            " 'Loss/regularization_loss': 0.18028587,\n",
            " 'Loss/total_loss': 0.20241046,\n",
            " 'learning_rate': 0.04}\n",
            "I0915 13:48:53.537069 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.01927965,\n",
            " 'Loss/localization_loss': 0.0028449497,\n",
            " 'Loss/regularization_loss': 0.18028587,\n",
            " 'Loss/total_loss': 0.20241046,\n",
            " 'learning_rate': 0.04}\n",
            "INFO:tensorflow:Step 2100 per-step time 2.919s\n",
            "I0915 13:53:45.429441 139846094190464 model_lib_v2.py:700] Step 2100 per-step time 2.919s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.021329489,\n",
            " 'Loss/localization_loss': 0.0036692559,\n",
            " 'Loss/regularization_loss': 0.17511643,\n",
            " 'Loss/total_loss': 0.20011519,\n",
            " 'learning_rate': 0.039998136}\n",
            "I0915 13:53:45.429859 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.021329489,\n",
            " 'Loss/localization_loss': 0.0036692559,\n",
            " 'Loss/regularization_loss': 0.17511643,\n",
            " 'Loss/total_loss': 0.20011519,\n",
            " 'learning_rate': 0.039998136}\n",
            "INFO:tensorflow:Step 2200 per-step time 2.897s\n",
            "I0915 13:58:35.088903 139846094190464 model_lib_v2.py:700] Step 2200 per-step time 2.897s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.016098727,\n",
            " 'Loss/localization_loss': 0.0026852458,\n",
            " 'Loss/regularization_loss': 0.17009415,\n",
            " 'Loss/total_loss': 0.18887812,\n",
            " 'learning_rate': 0.039992537}\n",
            "I0915 13:58:35.089307 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.016098727,\n",
            " 'Loss/localization_loss': 0.0026852458,\n",
            " 'Loss/regularization_loss': 0.17009415,\n",
            " 'Loss/total_loss': 0.18887812,\n",
            " 'learning_rate': 0.039992537}\n",
            "INFO:tensorflow:Step 2300 per-step time 2.903s\n",
            "I0915 14:03:25.404157 139846094190464 model_lib_v2.py:700] Step 2300 per-step time 2.903s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.0151267685,\n",
            " 'Loss/localization_loss': 0.00388394,\n",
            " 'Loss/regularization_loss': 0.16533628,\n",
            " 'Loss/total_loss': 0.18434699,\n",
            " 'learning_rate': 0.03998321}\n",
            "I0915 14:03:25.404594 139846094190464 model_lib_v2.py:701] {'Loss/classification_loss': 0.0151267685,\n",
            " 'Loss/localization_loss': 0.00388394,\n",
            " 'Loss/regularization_loss': 0.16533628,\n",
            " 'Loss/total_loss': 0.18434699,\n",
            " 'learning_rate': 0.03998321}\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgIqEGCoVtZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66cd7b9c-1922-422a-f639-fbf2b5317c5f"
      },
      "source": [
        "!python exporter_main_v2.py --input_type image_tensor --pipeline_config_path /content/workspace/training_demo/models/my_ssd_resnet50_v1_fpn/pipeline.config --trained_checkpoint_dir /content/workspace/training_demo/models/my_ssd_resnet50_v1_fpn --output_directory /content/workspace/training_demo/exported-models"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-09-15 14:08:22.462542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 14:08:22.532853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 14:08:22.533682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 14:08:22.556257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 14:08:22.557086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 14:08:22.557933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 14:08:27.248993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 14:08:27.250041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 14:08:27.250876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-15 14:08:27.251663: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-09-15 14:08:27.251738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10819 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:463: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.map_fn(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
            "W0915 14:08:27.485897 140077832120192 deprecation.py:616] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:463: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.map_fn(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f6600398890>, because it is not built.\n",
            "W0915 14:08:50.057539 140077832120192 save_impl.py:72] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f6600398890>, because it is not built.\n",
            "2021-09-15 14:09:04.079445: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "W0915 14:09:29.793581 140077832120192 save.py:254] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxPredictor_layer_call_fn while saving (showing 5 of 520). These functions will not be directly callable after loading.\n",
            "INFO:tensorflow:Assets written to: /content/workspace/training_demo/exported-models/saved_model/assets\n",
            "I0915 14:09:37.097595 140077832120192 builder_impl.py:781] Assets written to: /content/workspace/training_demo/exported-models/saved_model/assets\n",
            "INFO:tensorflow:Writing pipeline config file to /content/workspace/training_demo/exported-models/pipeline.config\n",
            "I0915 14:09:38.045513 140077832120192 config_util.py:254] Writing pipeline config file to /content/workspace/training_demo/exported-models/pipeline.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0ByulTuzbEF",
        "outputId": "21e43cc2-8f8d-43cb-f9f4-cddac035a2da"
      },
      "source": [
        "cd /content/workspace/training_demo/exported-models"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/workspace/training_demo/exported-models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0bQs-F1Lzewa",
        "outputId": "768c8b86-d609-4a88-82a0-6ba8c105a0e2"
      },
      "source": [
        "import shutil\n",
        "shutil.make_archive('sky', 'zip', '/content/workspace/training_demo/exported-models')\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/workspace/training_demo/exported-models/sky.zip'"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VyYb2nb2li2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc1_GLO32mEO"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "using webcame in google collab\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5drwW7_2tjK"
      },
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt05Tct92to-"
      },
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekPXyPtC23pV"
      },
      "source": [
        "**bold text**## Haar Cascade Classifier\n",
        "For this tutorial we will run a simple object detection algorithm called Haar Cascade on our images and video fetched from our webcam. OpenCV has a pre-trained Haar Cascade face detection model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7UnOXCe25sh"
      },
      "source": [
        "# initialize the Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt5Jjc8d3GPB"
      },
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # get photo data\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # get OpenCV format image\n",
        "  img = js_to_image(data) \n",
        "  # grayscale img\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  print(gray.shape)\n",
        "  # get face bounding box coordinates using Haar Cascade\n",
        "  faces = face_cascade.detectMultiScale(gray)\n",
        "  # draw face bounding box on image\n",
        "  for (x,y,w,h) in faces:\n",
        "      img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "  # save image\n",
        "  cv2.imwrite(filename, img)\n",
        "\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26QlV-RD3H6T"
      },
      "source": [
        "try:\n",
        "  filename = take_photo('photo.jpg')\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQCc1PHF3LB_"
      },
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsnwAJNY3M4A"
      },
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # grayscale image for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # get face region coordinates\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "    # get face bounding box for overlay\n",
        "    for (x,y,w,h) in faces:\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}